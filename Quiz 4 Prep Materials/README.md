## Quiz 4 Information

**Opens**: March 18th at 8am ET  
**Covers**: Structured Representations (Lesson 11), Language Models (Lesson 12), Embeddings (Lesson 13), Neural Attention Models (Lesson 14), Neural Machine Translation (Lesson 15), and Advanced Topics (Lesson 16).

### Focus Topics:

#### Conceptual Questions:

- RNNs and LSTMs, how their update rules differ, and what problems they each have or solve
- Conditional language models and how to train them (teacher/student forcing), language metrics (how to calculate them), how knowledge distillation works
- Word embeddings and details of word2vec/skip-gram model (form of probability equation, what is conditioned on what, intrinsic/extrinsic evaluation)
- What t-SNE is and how it conceptually works
- The encoder-decoder model and how it's used by recurrent neural networks for seq2seq prediction problems
- Beam search
- Neural attention (different types, how it's computed); "Attention is all you Need"
- Transformers (architecture details, positional encoding, computational complexity)
- Byte pair encoding

#### Computation Questions:

- Be familiar with how to calculate the sizes of the different self-attention matrices in the encoder, decoder, and between the encoder and decoder (often called cross-attention) based on "Attention is all you Need."

**Note**: The breakdown is ~20% computation and the rest is conceptual. This quiz covers all of module 3.
